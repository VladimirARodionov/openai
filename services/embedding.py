import logging
from datetime import timedelta
from pathlib import Path

import openai
import tiktoken
from llama_cloud import MessageRole
from llama_index.core import Settings, StorageContext, SimpleDirectoryReader, VectorStoreIndex, PromptTemplate, Document, SummaryIndex
from llama_index.core.base.llms.types import ChatMessage
from llama_index.core.node_parser import SimpleNodeParser
from llama_index.core.query_engine import CitationQueryEngine
from llama_index.core.response_synthesizers import ResponseMode
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.couchbase import CouchbaseVectorStore
from couchbase.auth import PasswordAuthenticator
from couchbase.cluster import Cluster
from couchbase.options import ClusterOptions, ClusterTimeoutOptions
from llama_index.llms.openai import OpenAI
from llama_index.core.prompts import ChatPromptTemplate

from create_bot import env_config
from services.common import get_search_from_inet

logger = logging.getLogger(__name__)

def read_from_file(file_path) -> str:
    with open(file_path, "r", encoding="utf-8") as f:
        return "".join(f.readlines())


# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤ OpenAI
original_post = openai.OpenAI.post
def logging_post(self, *args, **kwargs):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–ª–∞–≥ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ç–µ–∫—É—â–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä–µ
    if hasattr(self, '_disable_logging') and self._disable_logging:
        return original_post(self, *args, **kwargs)
    
    logger.info(f"OpenAI request args: {args} {kwargs}")
    response = original_post(self, *args, **kwargs)
    logger.info(f"OpenAI response: {response}")
    return response

openai.OpenAI.post = logging_post

# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —à–∞–±–ª–æ–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —á–∞—Ç–∞

TEXT_QA_SYSTEM_PROMPT = ChatMessage(
    content=(
        "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é. "
        "–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏ '–ù–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.'"
    ),
    role=MessageRole.SYSTEM,
)

TEXT_QA_PROMPT_TMPL_MSGS = [
    TEXT_QA_SYSTEM_PROMPT,
    ChatMessage(
        content=(
            "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n"
            "---------------------\n"
            "{context_str}\n"
            "---------------------\n"
            "–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ. "
            "–í–æ–ø—Ä–æ—Å: {query_str}\n"
            "–û—Ç–≤–µ—Ç: "
        ),
        role=MessageRole.USER,
    ),
]

# –°–æ–∑–¥–∞–µ–º —à–∞–±–ª–æ–Ω —á–∞—Ç–∞
CHAT_TEXT_QA_PROMPT = ChatPromptTemplate(message_templates=TEXT_QA_PROMPT_TMPL_MSGS)

CITATION_QA_TEMPLATE = PromptTemplate(read_from_file("templates/citation_qa_template.txt"))
CITATION_REFINE_TEMPLATE = PromptTemplate(read_from_file("templates/citation_refine_template.txt"))

CITATION_QA_TEMPLATE_INTERNET = PromptTemplate(read_from_file("templates/citation_qa_template_internet.txt"))

CITATION_REFINE_TEMPLATE_INTERNET = PromptTemplate(read_from_file("templates/citation_refine_template_internet.txt"))

INTERNET_QA_TEMPLATE = PromptTemplate(read_from_file("templates/internet_qa_template.txt"))
INTERNET_REPORT_TEMPLATE = PromptTemplate(read_from_file("templates/internet_report_template.txt"))

def _create_query_engine(index, top_k:int = 20):
    query_engine = CitationQueryEngine.from_args(
        index,
        citation_chunk_size=1024,
        similarity_top_k=top_k or env_config.get('SIMILARITY_TOP_K', 10),
        citation_qa_template=CITATION_QA_TEMPLATE,
        citation_refine_template=CITATION_REFINE_TEMPLATE,
        response_mode=ResponseMode.COMPACT
    )
    return query_engine


def _extract_topics(text: str) -> list[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º –∏–∑ —Ç–µ–∫—Å—Ç–∞

    Args:
        text (str): –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º—ã–π —Ç–µ–∫—Å—Ç

    Returns:
        list[str]: –°–ø–∏—Å–æ–∫ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º
    """
    try:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è —Ç–µ–º
        prompt = PromptTemplate(
            "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –∏ –≤—ã–¥–µ–ª–∏ 3-5 –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º –∏–ª–∏ –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–Ω—è—Ç–∏–π:\n"
            "{text}\n"
            "–¢–µ–º—ã:"
        )

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º GPT –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        response = Settings.llm.complete(prompt.format(text=text[:1000]))  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—Å—Ç–∞

        # –†–∞–∑–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç
        topics = [topic.strip().lower() for topic in str(response).split('\n') if topic.strip()]

        return topics

    except Exception as e:
        logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–º: {str(e)}")
        return []


class EmbeddingsSearch:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å API –∫–ª—é—á–æ–º OpenAI –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ Couchbase"""
        self.EMBEDDING_MODEL = env_config.get('EMBEDDING_MODEL')
        self.GPT_MODEL = env_config.get('MODEL')
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º API –∫–ª—é—á OpenAI
        openai_api_key = env_config.get('OPEN_AI_TOKEN')
        if not openai_api_key:
            raise ValueError("OpenAI API key not found in environment variables")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è llama-index settings —Å API –∫–ª—é—á–æ–º
        Settings.llm = OpenAI(
            model=self.GPT_MODEL,
            api_key=openai_api_key,
            max_retries=2,
            timeout=30,
            request_timeout=30
        )
        Settings.embed_model = OpenAIEmbedding(
            model=self.EMBEDDING_MODEL,
            api_key=openai_api_key,
            max_retries=2,
            timeout=30,
            request_timeout=30
        )

        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
        couchbase_host = env_config.get('COUCHBASE_HOST')

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º connection string —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –≤—Å–µ—Ö –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ—Ä—Ç–æ–≤
        connection_string = f"couchbase://{couchbase_host}"
        self.cluster = Cluster.connect(
            connection_string,
            ClusterOptions(PasswordAuthenticator(env_config.get('COUCHBASE_ADMINISTRATOR_USERNAME'),
                                                 env_config.get('COUCHBASE_ADMINISTRATOR_PASSWORD')),
                           timeout_options=ClusterTimeoutOptions(
                               kv_timeout=timedelta(seconds=120),
                               query_timeout=timedelta(seconds=120),
                               search_timeout=timedelta(seconds=120)
                           ))
        )
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
            mgr = self.cluster.search_indexes()
            indexes = mgr.get_all_indexes()
            logger.info(f"Available indexes: {[idx.name for idx in indexes]}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º GSI –∏–Ω–¥–µ–∫—Å—ã
            result = self.cluster.query(
                "SELECT * FROM system:indexes;"
            )
            gsi_indexes = [row for row in result]
            logger.info(f"Available GSI indexes in _default scope: {gsi_indexes}")
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
            self.vector_store = CouchbaseVectorStore(
                cluster=self.cluster,
                bucket_name="vector_store",
                scope_name="_default",
                collection_name="_default",
                index_name="vector-index"
            )
            logger.info("Vector store initialized successfully")
            
        except Exception as e:
            logger.error(f"Error initializing vector store: {str(e)}")
            raise
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è storage context
        self.storage_context = StorageContext.from_defaults(
            vector_store=self.vector_store
        )
        self.node_parser = SimpleNodeParser.from_defaults()

    def num_tokens(self, text):
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        encoding = tiktoken.encoding_for_model(self.GPT_MODEL)
        return len(encoding.encode(text))

    def _split_text_into_paragraphs(self, text: str) -> list[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        
        Args:
            text (str): –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç

        Returns:
            list[str]: –°–ø–∏—Å–æ–∫ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
        """
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫
        paragraphs = [p.strip() for p in text.split('\n\n')]
        
        return paragraphs

    def load_documents_from_directory(self, directory_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏, —Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Couchbase
        
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã:
        - –¢–µ–∫—Å—Ç–æ–≤—ã–µ: .txt, .md, .json, .csv, .html, .xml, .pdf, .doc, .docx, .ppt, .pptx
        - –ê—É–¥–∏–æ: .mp3, .mp4, .mpeg, .mpga, .m4a, .wav, .webm
        - –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: .jpg, .jpeg, .png, .gif, .webp
        - –ö–æ–¥: .py, .js, .java, .cpp, .h, .c, .cs, .php, .rb, .swift, .go
        
        Args:
            directory_path (str): –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        """
        try:
            # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ OpenAI –∑–∞–ø—Ä–æ—Å–æ–≤
            openai.OpenAI._disable_logging = True
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º SimpleDirectoryReader –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤—Å–µ—Ö –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
            documents = SimpleDirectoryReader(
                input_dir=directory_path,
                recursive=True,
                filename_as_id=True,
                required_exts=[
                    # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã
                    ".txt", ".md", ".json", ".csv", ".html", ".xml",
                    ".pdf", ".doc", ".docx", ".ppt", ".pptx",
                    # –ê—É–¥–∏–æ —Ñ–æ—Ä–º–∞—Ç—ã
                    ".mp3", ".mp4", ".mpeg", ".mpga", ".m4a", ".wav", ".webm",
                    # –§–æ—Ä–º–∞—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                    ".jpg", ".jpeg", ".png", ".gif", ".webp",
                    # –§–æ—Ä–º–∞—Ç—ã –∫–æ–¥–∞
                    ".py", ".js", ".java", ".cpp", ".h", ".c", ".cs", ".php", ".rb", ".swift", ".go"
                ],
                exclude_hidden=True
            ).load_data()

            if documents:
                # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                for doc in documents:
                    file_path = Path(doc.metadata.get('file_path', ''))
                    if file_path:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–µ–∑ –ø—É—Ç–∏ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –∫–∞–∫ –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞
                        doc.doc_id = file_path.stem
                        doc.metadata.update({
                            "file_type": file_path.suffix.lower().lstrip('.'),
                            "file_name": file_path.name,
                            "source": file_path.stem,
                            "type": "vector"
                        })

                # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ü–µ–ª–∏–∫–æ–º
                index = VectorStoreIndex.from_documents(
                    documents,
                    storage_context=self.storage_context,
                    show_progress=True
                )

                file_count = len(documents)
                logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {file_count}")
                
                # –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
                openai.OpenAI._disable_logging = False
                
                return f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {file_count} —Ñ–∞–π–ª–æ–≤"
            else:
                # –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ
                openai.OpenAI._disable_logging = False
                return "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"
            
        except Exception as e:
            # –í–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ –¥–∞–∂–µ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            openai.OpenAI._disable_logging = False
            logger.exception(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}")
            return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {str(e)}"

    def ask(self, query, user_id, print_message=False):
        """–û—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT –∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        try:
            search_from_inet = get_search_from_inet(user_id)
            response_parts = []
            
            # 1. –ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            index = VectorStoreIndex.from_vector_store(self.vector_store)
            query_engine = _create_query_engine(index)
            local_response = query_engine.query(query)
            response_parts.append(str(local_response))
            
            # 2. –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —á–µ—Ä–µ–∑ GPT, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
            if search_from_inet:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —à–∞–±–ª–æ–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ
                    internet_response = Settings.llm.complete(
                        INTERNET_QA_TEMPLATE.format(
                            query_str=query,
                            local_response=str(local_response)
                        )
                    )
                    
                    if str(internet_response).strip():
                        response_parts.append("\n\nüåê –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:\n" + str(internet_response))
                
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {str(e)}")
                    response_parts.append("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞")
            
            if print_message:
                logger.info(f"Query: {query}")
                logger.info(f"Response: {response_parts}")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            logger.exception(str(e))
            return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"

    def report(self, query: str, user_id, print_message=False):
        """–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        try:
            search_from_inet = get_search_from_inet(user_id)
            report_parts = []
            
            # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
            index = VectorStoreIndex.from_vector_store(self.vector_store)
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –Ω–æ–¥—ã —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            retriever = index.as_retriever(similarity_top_k=10)
            nodes = retriever.retrieve(query)
            
            # 1. –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            query_engine = _create_query_engine(index)
            main_response = query_engine.query(query)
            report_parts.append(f"üîç –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n\n{str(main_response)}\n")
            
            # 2. –ö—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if nodes:
                documents = [
                    Document(
                        text=node.text,
                        metadata=node.metadata
                    ) for node in nodes
                ]
                
                summary_index = SummaryIndex.from_documents(documents)
                summary = summary_index.as_query_engine().query(
                    "–°–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"
                )
                report_parts.append(f"\nüìù –ö—Ä–∞—Ç–∫–æ–µ —Å–∞–º–º–∞—Ä–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n{str(summary)}\n")
            
            # 3. –ü–æ–∏—Å–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ —á–µ—Ä–µ–∑ GPT, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
            if search_from_inet:
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —à–∞–±–ª–æ–Ω –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
                    internet_response = Settings.llm.complete(
                        INTERNET_REPORT_TEMPLATE.format(
                            query_str=query,
                            local_info=str(main_response)
                        )
                    )
                    
                    if str(internet_response).strip():
                        report_parts.append("\nüåê –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞:\n" + str(internet_response))
                
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ: {str(e)}")
                    report_parts.append("\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞")
            
            # 4. –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            sources = {}
            for node in nodes:
                source = node.metadata.get('source', 'Unknown')
                sources[source] = sources.get(source, 0) + 1
            
            report_parts.append("\nüìö –û—Å–Ω–æ–≤–Ω—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:")
            for source, count in sorted(sources.items(), key=lambda x: x[1], reverse=True)[:5]:
                report_parts.append(f"- {source}: {count} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            
            return "\n".join(report_parts)
            
        except Exception as e:
            logger.exception(str(e))
            return f"–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {str(e)}"

    def clear_database(self):
        """–û—á–∏—Å—Ç–∫–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø –∫ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
            bucket = self.cluster.bucket(self.vector_store._bucket_name)
            scope = bucket.scope(self.vector_store._scope_name)
            collection = scope.collection(self.vector_store._collection_name)
            
            # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            count_query = f"SELECT COUNT(*) as count FROM `{self.vector_store._bucket_name}`.`{self.vector_store._scope_name}`.`{self.vector_store._collection_name}`"
            result = self.cluster.query(count_query).rows()
            initial_count = next(result)['count']
            logger.info(f"Documents before deletion: {initial_count}")
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            id_query = f"SELECT META().id FROM `{self.vector_store._bucket_name}`.`{self.vector_store._scope_name}`.`{self.vector_store._collection_name}`"
            result = self.cluster.query(id_query).rows()
            
            # –£–¥–∞–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É
            deleted_count = 0
            for row in result:
                try:
                    collection.remove(row['id'])
                    deleted_count += 1
                except Exception as e:
                    logger.error(f"Error deleting document {row['id']}: {str(e)}")
            
            logger.info(f"Deleted {deleted_count} documents")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã —É–¥–∞–ª–µ–Ω—ã
            result = self.cluster.query(count_query).rows()
            final_count = next(result)['count']
            logger.info(f"Documents after deletion: {final_count}")
            
            if final_count == 0:
                logger.info("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —É—Å–ø–µ—à–Ω–æ –æ—á–∏—â–µ–Ω–∞")
                return "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ—á–∏—â–µ–Ω–∞"
            else:
                error_msg = f"–ù–µ –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –±—ã–ª–∏ —É–¥–∞–ª–µ–Ω—ã. –û—Å—Ç–∞–ª–æ—Å—å: {final_count}"
                logger.error(error_msg)
                return error_msg
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—á–∏—Å—Ç–∫–µ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {str(e)}"
            logger.exception(error_msg)
            return error_msg
